{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "257d4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48acba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Setting up directories...\n",
      "‚ö†Ô∏è Cleaning up partial/failed downloads from previous run...\n",
      "‚¨áÔ∏è Downloading the Zip file (Much faster for 9000 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hLyTHzlVOAWCzK6VId-Tl-O39ZQUKxCs\n",
      "To: /content/datasets/temp_data.zip\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.1M/12.1M [00:00<00:00, 45.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Unzipping files...\n",
      "‚úÖ Success! Unzipped 2 files into /content/datasets/parsed_windows_pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 1. PASTE THE LINK TO YOUR NEW .ZIP FILE HERE (Not the folder link!)\n",
    "zip_file_url = \"https://drive.google.com/file/d/1hLyTHzlVOAWCzK6VId-Tl-O39ZQUKxCs/view?usp=sharing\"\n",
    "\n",
    "# 2. Paths\n",
    "BASE_DIR = Path(os.getcwd())\n",
    "DATASETS_DIR = BASE_DIR / \"datasets\"\n",
    "INPUT_DIR = DATASETS_DIR / \"parsed_windows_pdf\"\n",
    "ZIP_OUTPUT = DATASETS_DIR / \"temp_data.zip\"\n",
    "\n",
    "# --- LOGIC ---\n",
    "print(f\"üìÇ Setting up directories...\")\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Clean up any failed previous attempts\n",
    "if any(INPUT_DIR.iterdir()):\n",
    "    print(\"‚ö†Ô∏è Cleaning up partial/failed downloads from previous run...\")\n",
    "    shutil.rmtree(INPUT_DIR)\n",
    "    INPUT_DIR.mkdir()\n",
    "\n",
    "print(\"‚¨áÔ∏è Downloading the Zip file (Much faster for 9000 files)...\")\n",
    "\n",
    "try:\n",
    "    # Download the single zip file\n",
    "    # fuzzy=True helps it handle different Google Drive URL formats\n",
    "    gdown.download(url=zip_file_url, output=str(ZIP_OUTPUT), quiet=False, fuzzy=True)\n",
    "    \n",
    "    print(\"üì¶ Unzipping files...\")\n",
    "    with zipfile.ZipFile(ZIP_OUTPUT, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATASETS_DIR) # Extracts into datasets/\n",
    "    \n",
    "    # Clean up the zip file to save space\n",
    "    os.remove(ZIP_OUTPUT)\n",
    "    \n",
    "    # Verify\n",
    "    file_count = len(list(INPUT_DIR.glob('*')))\n",
    "    print(f\"‚úÖ Success! Unzipped {file_count} files into {INPUT_DIR}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed: {e}\")\n",
    "    print(\"üí° Check that you pasted the link to the .zip file, not the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf19b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading Qwen2.5-7B with 4-bit quantization...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(\"üîß Loading Qwen2.5-7B with 4-bit quantization...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6c11ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a8bafed61248f1a6c05bc34a484729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded! VRAM usage:\")\n",
    "!nvidia-smi --query-gpu=memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "MAX_NEW_TOKENS = 800\n",
    "SAVE_INTERVAL = 10  # Save checkpoint every 10 conversations\n",
    "\n",
    "def parse_txt_file(filepath):\n",
    "    \"\"\"Parse the formatted .txt file and extract structured data.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Extract ID\n",
    "    id_match = re.search(r'CALL WINDOW:\\s*(\\S+)\\.json', content)\n",
    "    if id_match:\n",
    "        data['id'] = id_match.group(1)\n",
    "    \n",
    "    # Extract Split\n",
    "    split_match = re.search(r'Split:\\s*(\\w+)', content)\n",
    "    if split_match:\n",
    "        data['split'] = split_match.group(1).lower()\n",
    "    \n",
    "    # Extract Label\n",
    "    label_match = re.search(r'Label:\\s*(\\w+)', content)\n",
    "    if label_match:\n",
    "        data['label_name'] = label_match.group(1).lower()\n",
    "        data['label'] = 1 if data['label_name'] == 'scam' else 0\n",
    "    \n",
    "    # Extract Call ID\n",
    "    call_id_match = re.search(r'Call ID:\\s*(\\d+)', content)\n",
    "    if call_id_match:\n",
    "        data['source_idx'] = int(call_id_match.group(1))\n",
    "    \n",
    "    # Extract Window\n",
    "    window_match = re.search(r'Window:\\s*w(\\d+)', content)\n",
    "    if window_match:\n",
    "        data['window_index'] = int(window_match.group(1))\n",
    "    \n",
    "    # Extract SOURCE_IDX from content\n",
    "    source_idx_match = re.search(r'SOURCE_IDX:\\s*(\\d+)', content)\n",
    "    if source_idx_match:\n",
    "        data['source_idx'] = int(source_idx_match.group(1))\n",
    "    \n",
    "    # Extract PREVIOUS_STATE\n",
    "    prev_state_match = re.search(r'PREVIOUS_STATE:\\s*\\n(.*?)\\n\\nCURRENT_WINDOW:', content, re.DOTALL)\n",
    "    data['previous_state'] = prev_state_match.group(1).strip() if prev_state_match else \"\"\n",
    "    \n",
    "    # Extract CURRENT_WINDOW\n",
    "    window_match = re.search(r'CURRENT_WINDOW:\\s*\\n(\\[.*?\\])', content, re.DOTALL)\n",
    "    if window_match:\n",
    "        try:\n",
    "            data['current_window'] = json.loads(window_match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            data['current_window'] = []\n",
    "    \n",
    "    # Extract METADATA\n",
    "    metadata_match = re.search(r'METADATA:\\s*\\n(\\{.*?\\})', content, re.DOTALL)\n",
    "    if metadata_match:\n",
    "        try:\n",
    "            data['metadata'] = json.loads(metadata_match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            data['metadata'] = {}\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def format_conversation_for_prompt(current_window):\n",
    "    \"\"\"Format conversation turns into readable string.\"\"\"\n",
    "    formatted = []\n",
    "    for turn in current_window:\n",
    "        speaker = turn.get('speaker', 'Unknown')\n",
    "        text = turn.get('text', '')\n",
    "        formatted.append(f\"Speaker: {speaker}\\nText: {text}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def build_system_prompt():\n",
    "    return \"\"\"You are SixEyes, an AI Security Analyst specializing in phone scam detection research. You are analyzing a labeled dataset of phone call transcripts to build a real-time fraud detection system.\n",
    "\n",
    "Your role:\n",
    "- Analyze phone conversation segments for fraud indicators\n",
    "- Identify manipulation tactics, pressure techniques, and social engineering patterns\n",
    "- Provide structured analysis to train fraud detection models\n",
    "- Help protect vulnerable individuals from phone scams\n",
    "\n",
    "This is legitimate security research using pre-labeled data. Your analysis helps prevent fraud.\"\"\"\n",
    "\n",
    "\n",
    "def build_user_prompt(data, running_state_context):\n",
    "    is_scam = data['label'] == 1\n",
    "    label_context = \"SCAM (labeled by security researchers)\" if is_scam else \"LEGITIMATE (labeled by security researchers)\"\n",
    "    conversation_text = format_conversation_for_prompt(data['current_window'])\n",
    "    \n",
    "    prompt = f\"\"\"DATASET LABEL: {label_context}\n",
    "\n",
    "CONTEXT FROM PREVIOUS ANALYSIS:\n",
    "{running_state_context if running_state_context else \"Start of call. No previous context.\"}\n",
    "\n",
    "CURRENT CONVERSATION SEGMENT:\n",
    "{conversation_text}\n",
    "\n",
    "ANALYSIS TASK:\n",
    "Analyze this conversation segment and provide your findings in the following XML format:\n",
    "\n",
    "<THOUGHT>\n",
    "[Analyze the fraud indicators, manipulation tactics, urgency creation, impersonation attempts, or normal business patterns. Be specific about what you observe.]\n",
    "</THOUGHT>\n",
    "\n",
    "<STATE>\n",
    "[Summarize the call state SO FAR: key entities mentioned, claimed authority/identity, threats or demands made, urgency level, risk score 1-10. This will be passed to analyze the next segment.]\n",
    "</STATE>\n",
    "\n",
    "<VERDICT>\n",
    "[Choose ONE: SAFE, SUSPICIOUS, or DANGER based on the patterns observed]\n",
    "</VERDICT>\n",
    "\n",
    "Provide your analysis now:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_response(system_prompt, user_prompt):\n",
    "    \"\"\"Generate response using Qwen2.5-7B.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def extract_xml_content(response_text, tag_name):\n",
    "    pattern = f\"<{tag_name}>(.*?)</{tag_name}>\"\n",
    "    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def save_augmented_json(data, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def group_files_by_conversation(input_dir):\n",
    "    conversations = defaultdict(list)\n",
    "    for category in ['legit_windows', 'scam_windows']:\n",
    "        category_path = Path(input_dir) / category\n",
    "        if not category_path.exists(): \n",
    "            continue\n",
    "        \n",
    "        for txt_file in category_path.glob(\"*.txt\"):\n",
    "            try:\n",
    "                data = parse_txt_file(txt_file)\n",
    "                if data.get('source_idx') is not None:\n",
    "                    conversations[data['source_idx']].append({\n",
    "                        'filepath': txt_file,\n",
    "                        'window_index': data['window_index'],\n",
    "                        'data': data\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Error parsing {txt_file}: {e}\")\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def process_single_conversation(source_idx, windows):\n",
    "    \"\"\"Process all windows of a conversation sequentially.\"\"\"\n",
    "    system_prompt = build_system_prompt()\n",
    "    windows.sort(key=lambda x: x['window_index'])\n",
    "    \n",
    "    running_state_context = \"\"\n",
    "    processed = 0\n",
    "    \n",
    "    for window_info in windows:\n",
    "        data = window_info['data']\n",
    "        window_index = window_info['window_index']\n",
    "        \n",
    "        # Check if already processed\n",
    "        split_name = data['split']\n",
    "        label_name = data['label_name']\n",
    "        output_path = OUTPUT_DIR / split_name / label_name / f\"{data['id']}.json\"\n",
    "        \n",
    "        if output_path.exists():\n",
    "            try:\n",
    "                with open(output_path, 'r') as f:\n",
    "                    existing_data = json.load(f)\n",
    "                    if existing_data.get('generated_state'):\n",
    "                        running_state_context = existing_data['generated_state']\n",
    "                processed += 1\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Generate analysis\n",
    "        user_prompt = build_user_prompt(data, running_state_context)\n",
    "        response_text = generate_response(system_prompt, user_prompt)\n",
    "        \n",
    "        thought = extract_xml_content(response_text, \"THOUGHT\")\n",
    "        state = extract_xml_content(response_text, \"STATE\")\n",
    "        verdict = extract_xml_content(response_text, \"VERDICT\")\n",
    "        \n",
    "        running_state_context = state if state else running_state_context\n",
    "        \n",
    "        augmented_data = {\n",
    "            \"id\": data['id'],\n",
    "            \"source_idx\": data['source_idx'],\n",
    "            \"split\": data['split'],\n",
    "            \"label\": data['label'],\n",
    "            \"label_name\": data['label_name'],\n",
    "            \"window_index\": window_index,\n",
    "            \"current_window\": data['current_window'],\n",
    "            \"metadata\": data.get('metadata', {}),\n",
    "            \"model_input_context\": running_state_context,\n",
    "            \"generated_thought\": thought,\n",
    "            \"generated_state\": state,\n",
    "            \"generated_verdict\": verdict,\n",
    "            \"raw_response\": response_text\n",
    "        }\n",
    "        \n",
    "        save_augmented_json(augmented_data, output_path)\n",
    "        processed += 1\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d55c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Processing...\n",
      "üìÇ Found 0 conversations\n",
      "‚ö†Ô∏è  TEST MODE: Processing only 0 conversations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33ef7ec502947c69c447a84647fecc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Complete! Processed 0 windows in 0.0 minutes\n",
      "‚ö° Speed: 0.00 windows/sec\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 6: Verify Output\n",
    "print(\"üöÄ Starting Processing...\")\n",
    "conversations = group_files_by_conversation(INPUT_DIR)\n",
    "total_conversations = len(conversations)\n",
    "print(f\"üìÇ Found {total_conversations} conversations\")\n",
    "\n",
    "# Test mode toggle\n",
    "TEST_MODE = True  # ‚ö†Ô∏è Set to False for full run\n",
    "if TEST_MODE:\n",
    "    conversations = dict(list(conversations.items())[:50])\n",
    "    print(f\"‚ö†Ô∏è  TEST MODE: Processing only {len(conversations)} conversations\")\n",
    "\n",
    "start_time = time.time()\n",
    "total_windows = 0\n",
    "processed_convs = 0\n",
    "\n",
    "for source_idx, windows in tqdm(conversations.items(), desc=\"Processing\"):\n",
    "    windows_processed = process_single_conversation(source_idx, windows)\n",
    "    total_windows += windows_processed\n",
    "    processed_convs += 1\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if processed_convs % SAVE_INTERVAL == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        speed = total_windows / elapsed\n",
    "        eta = (len(conversations) - processed_convs) * (elapsed / processed_convs)\n",
    "        print(f\"üíæ Checkpoint: {processed_convs}/{len(conversations)} convs | {speed:.2f} win/sec | ETA: {eta/60:.1f}min\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚ú® Complete! Processed {total_windows} windows in {elapsed/60:.1f} minutes\")\n",
    "print(f\"‚ö° Speed: {total_windows/elapsed:.2f} windows/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
